{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/3, Total Reward: 32363166, Epsilon: 0.0997\n",
      "Episode 2/3, Total Reward: 16264675, Epsilon: 0.0997\n",
      "Episode 3/3, Total Reward: 59109766, Epsilon: 0.0997\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "class SugarscapeEnvironment:\n",
    "    def __init__(self, width, height, num_agents, params):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_agents = num_agents\n",
    "        self.params = params\n",
    "\n",
    "        self.job_centers = []\n",
    "        self.sugar = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.create_initial_sugar_peaks()\n",
    "        self.max_sugar_landscape = self.sugar.copy()\n",
    "        self.agents = self.initialize_agents()\n",
    "        self.agent_positions = set((agent['x'], agent['y']) for agent in self.agents)\n",
    "        self.dead_agents = []\n",
    "\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Initialize DQN components\n",
    "        self.observation_space_size = (self.params['vision_range'] * 2 + 1) ** 2 + 2  # Observation size\n",
    "        self.action_space_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.policy_net = DQN(self.observation_space_size, self.action_space_size)\n",
    "        self.target_net = DQN(self.observation_space_size, self.action_space_size)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.params['learning_rate'])\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = self.params['batch_size']\n",
    "        self.gamma = self.params['gamma']\n",
    "        self.epsilon = self.params['epsilon_start']\n",
    "\n",
    "    def create_initial_sugar_peaks(self, num_peaks=2):\n",
    "        for _ in range(num_peaks):\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "    def create_job_center(self):\n",
    "        x, y = np.random.randint(0, self.width), np.random.randint(0, self.height)\n",
    "        duration = np.random.randint(*self.params['job_center_duration'])\n",
    "        self.job_centers.append({\n",
    "            'x': x, 'y': y,\n",
    "            'duration': duration,\n",
    "            'max_sugar': self.params['max_sugar']\n",
    "        })\n",
    "\n",
    "    def update_sugar_landscape(self):\n",
    "        self.sugar = np.zeros((self.height, self.width))\n",
    "        for center in self.job_centers:\n",
    "            x_grid, y_grid = np.meshgrid(np.arange(self.width), np.arange(self.height))\n",
    "            distance = np.sqrt((x_grid - center['x']) ** 2 + (y_grid - center['y']) ** 2)\n",
    "            sugar_level = center['max_sugar'] * np.exp(-distance ** 2 / (2 * self.params['sugar_peak_spread'] ** 2))\n",
    "            self.sugar += sugar_level\n",
    "        self.sugar = np.clip(self.sugar, 0, self.params['max_sugar'])\n",
    "        self.sugar = np.round(self.sugar).astype(int)\n",
    "\n",
    "    def initialize_agents(self):\n",
    "        agents = []\n",
    "        available_positions = set((x, y) for x in range(self.width) for y in range(self.height))\n",
    "        for i in range(self.num_agents):\n",
    "            if not available_positions:\n",
    "                break\n",
    "            x, y = available_positions.pop()\n",
    "            agents.append(self.create_agent(i, x, y))\n",
    "        return agents\n",
    "\n",
    "    def create_agent(self, id, x, y):\n",
    "        return {\n",
    "            'id': id, 'x': x, 'y': y,\n",
    "            'sugar': np.random.randint(40, 80),\n",
    "            'metabolism': np.random.randint(1, 3),\n",
    "            'vision': np.random.randint(1, self.params['vision_range'] + 1),\n",
    "            'broadcast_radius': max(1, int(np.random.normal(self.params['broadcast_radius'], self.params['broadcast_radius'] / 3))),\n",
    "            'messages': deque(maxlen=100),\n",
    "            'destination': None,\n",
    "            'done': False\n",
    "        }\n",
    "\n",
    "    def get_agent_observation(self, agent):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        obs_range = self.params['vision_range']\n",
    "        # Get local sugar levels\n",
    "        sugar_obs = self.sugar[max(0, y - obs_range):min(self.height, y + obs_range + 1),\n",
    "                               max(0, x - obs_range):min(self.width, x + obs_range + 1)]\n",
    "        # Pad the observation if at the edges\n",
    "        pad_width_x = (max(0, obs_range - x), max(0, x + obs_range + 1 - self.width))\n",
    "        pad_width_y = (max(0, obs_range - y), max(0, y + obs_range + 1 - self.height))\n",
    "        sugar_obs = np.pad(sugar_obs, (pad_width_y, pad_width_x), mode='constant', constant_values=0)\n",
    "        sugar_obs = sugar_obs.flatten()\n",
    "        # Include agent's own sugar level and metabolism\n",
    "        agent_features = np.array([agent['sugar'], agent['metabolism']])\n",
    "        observation = np.concatenate((sugar_obs, agent_features))\n",
    "        return observation\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.randint(0, self.action_space_size - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                observation = torch.tensor(observation, dtype=torch.float32)\n",
    "                q_values = self.policy_net(observation)\n",
    "                action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, max(0, y - 1)),        # Up\n",
    "            1: (x, min(self.height - 1, y + 1)),  # Down\n",
    "            2: (max(0, x - 1), y),        # Left\n",
    "            3: (min(self.width - 1, x + 1), y),   # Right\n",
    "            4: (x, y)                     # Stay\n",
    "        }\n",
    "        new_x, new_y = possible_moves[action]\n",
    "        if (new_x, new_y) not in self.agent_positions:\n",
    "            self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            agent['x'], agent['y'] = new_x, new_y\n",
    "            self.agent_positions.add((new_x, new_y))\n",
    "        # else: If the space is occupied, the agent stays in place\n",
    "\n",
    "    def step(self):\n",
    "        # Update job centers\n",
    "        for center in self.job_centers:\n",
    "            center['duration'] -= 1\n",
    "        self.job_centers = [center for center in self.job_centers if center['duration'] > 0]\n",
    "        if np.random.random() < self.params['sugar_peak_frequency']:\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "        # For each agent, perform action and collect experience\n",
    "        for agent in self.agents:\n",
    "            observation = self.get_agent_observation(agent)\n",
    "            action = self.select_action(observation)\n",
    "            prev_x, prev_y = agent['x'], agent['y']\n",
    "            self.move_agent(agent, action)\n",
    "            collected_sugar = self.sugar[agent['y'], agent['x']]\n",
    "            agent['sugar'] += collected_sugar\n",
    "            self.sugar[agent['y'], agent['x']] = 0\n",
    "            agent['sugar'] -= agent['metabolism']\n",
    "            reward = collected_sugar - agent['metabolism']\n",
    "            done = agent['sugar'] <= 0\n",
    "\n",
    "            next_observation = self.get_agent_observation(agent)\n",
    "            experience = (observation, action, reward, next_observation, done)\n",
    "            self.memory.append(experience)\n",
    "\n",
    "            agent['done'] = done\n",
    "\n",
    "        # Update agent list\n",
    "        alive_agents = []\n",
    "        for agent in self.agents:\n",
    "            if agent['done']:\n",
    "                self.dead_agents.append({'x': agent['x'], 'y': agent['y'], 'death_time': self.timestep})\n",
    "                self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            else:\n",
    "                alive_agents.append(agent)\n",
    "        self.agents = alive_agents\n",
    "\n",
    "        # Perform learning step\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            self.learn()\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.params['epsilon_min']:\n",
    "            self.epsilon *= self.params['epsilon_decay']\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "    def learn(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        observations, actions, rewards, next_observations, dones = zip(*batch)\n",
    "\n",
    "        observations = torch.tensor(observations, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_observations = torch.tensor(next_observations, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        q_values = self.policy_net(observations).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_observations).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if self.timestep % self.params['target_update'] == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def train(self, num_episodes, max_timesteps):\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment\n",
    "            self.reset_environment()\n",
    "            episode_reward = 0\n",
    "            for timestep in range(max_timesteps):\n",
    "                self.step()\n",
    "                episode_reward += sum(agent['sugar'] for agent in self.agents)\n",
    "                if not self.agents:\n",
    "                    break\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {episode_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "            # Save model periodically\n",
    "            if (episode + 1) % self.params['save_interval'] == 0:\n",
    "                torch.save(self.policy_net.state_dict(), f\"policy_net_episode_{episode+1}.pth\")\n",
    "\n",
    "    def reset_environment(self):\n",
    "        self.job_centers = []\n",
    "        self.sugar = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.create_initial_sugar_peaks()\n",
    "        self.agents = self.initialize_agents()\n",
    "        self.agent_positions = set((agent['x'], agent['y']) for agent in self.agents)\n",
    "        self.dead_agents = []\n",
    "        self.timestep = 0\n",
    "\n",
    "# Training Parameters\n",
    "params = {\n",
    "    'max_sugar': 5,\n",
    "    'growth_rate': 1,\n",
    "    'sugar_peak_frequency': 0.04,\n",
    "    'sugar_peak_spread': 6,\n",
    "    'job_center_duration': (40, 100),\n",
    "    'vision_range': 1,\n",
    "    'message_expiry': 15,\n",
    "    'max_relay_messages': 10,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_min': 0.1,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 64,\n",
    "    'target_update': 100,\n",
    "    'broadcast_radius': 15,\n",
    "    'save_interval': 10  # Save model every 10 episodes\n",
    "}\n",
    "\n",
    "# Create environment and train\n",
    "env = SugarscapeEnvironment(width=50, height=50, num_agents=1000, params=params)\n",
    "num_episodes = 3\n",
    "max_timesteps = 1000\n",
    "\n",
    "env.train(num_episodes, max_timesteps)\n",
    "\n",
    "torch.save(env.policy_net.state_dict(), \"policy_net_final.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SugarscapeEnvironmentVisual' object has no attribute 'create_initial_sugar_peaks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 189\u001b[0m\n\u001b[0;32m    176\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_sugar\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrowth_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbroadcast_radius\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m    186\u001b[0m }\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Create environment and run simulation\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m env_visual \u001b[38;5;241m=\u001b[39m SugarscapeEnvironmentVisual(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_agents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    190\u001b[0m env_visual\u001b[38;5;241m.\u001b[39mrun_simulation(max_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mSugarscapeEnvironmentVisual.__init__\u001b[1;34m(self, width, height, num_agents, params)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_centers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msugar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_initial_sugar_peaks()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sugar_landscape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msugar\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_agents()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SugarscapeEnvironmentVisual' object has no attribute 'create_initial_sugar_peaks'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pygame\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "class SugarscapeEnvironmentVisual:\n",
    "    def __init__(self, width, height, num_agents, params):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_agents = num_agents\n",
    "        self.params = params\n",
    "\n",
    "        self.job_centers = []\n",
    "        self.sugar = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.create_initial_sugar_peaks()\n",
    "        self.max_sugar_landscape = self.sugar.copy()\n",
    "        self.agents = self.initialize_agents()\n",
    "        self.agent_positions = set((agent['x'], agent['y']) for agent in self.agents)\n",
    "        self.dead_agents = []\n",
    "\n",
    "        # Initialize PyGame\n",
    "        pygame.init()\n",
    "        self.cell_size = 10\n",
    "        self.screen = pygame.display.set_mode((width * self.cell_size, height * self.cell_size))\n",
    "        pygame.display.set_caption(\"Sugarscape Simulation - Execution\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.font = pygame.font.Font(None, 10)\n",
    "\n",
    "        self.population_history = []\n",
    "        self.average_wealth_history = []\n",
    "        self.gini_coefficient_history = []\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Load trained model\n",
    "        self.observation_space_size = (self.params['vision_range'] * 2 + 1) ** 2 + 2  # Observation size\n",
    "        self.action_space_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.policy_net = DQN(self.observation_space_size, self.action_space_size)\n",
    "        self.policy_net.load_state_dict(torch.load(\"policy_net_final.pth\"))\n",
    "        self.policy_net.eval()\n",
    "\n",
    "    # ... (Same methods as in the training environment, excluding learning parts)\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        with torch.no_grad():\n",
    "            observation = torch.tensor(observation, dtype=torch.float32)\n",
    "            q_values = self.policy_net(observation)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        # Update job centers\n",
    "        for center in self.job_centers:\n",
    "            center['duration'] -= 1\n",
    "        self.job_centers = [center for center in self.job_centers if center['duration'] > 0]\n",
    "        if np.random.random() < self.params['sugar_peak_frequency']:\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "        # For each agent, perform action\n",
    "        for agent in self.agents:\n",
    "            observation = self.get_agent_observation(agent)\n",
    "            action = self.select_action(observation)\n",
    "            self.move_agent(agent, action)\n",
    "            collected_sugar = self.sugar[agent['y'], agent['x']]\n",
    "            agent['sugar'] += collected_sugar\n",
    "            self.sugar[agent['y'], agent['x']] = 0\n",
    "            agent['sugar'] -= agent['metabolism']\n",
    "            agent['done'] = agent['sugar'] <= 0\n",
    "\n",
    "        # Update agent list\n",
    "        alive_agents = []\n",
    "        for agent in self.agents:\n",
    "            if agent['done']:\n",
    "                self.dead_agents.append({'x': agent['x'], 'y': agent['y'], 'death_time': self.timestep})\n",
    "                self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            else:\n",
    "                alive_agents.append(agent)\n",
    "        self.agents = alive_agents\n",
    "\n",
    "        self.dead_agents = [agent for agent in self.dead_agents if self.timestep - agent['death_time'] <= 5]\n",
    "\n",
    "        self.collect_data()\n",
    "        self.timestep += 1\n",
    "\n",
    "    def render(self):\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                sugar_level = self.sugar[y, x]\n",
    "                color = self.get_color(sugar_level)\n",
    "                pygame.draw.rect(self.screen, color,\n",
    "                                 (x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "        for dead_agent in self.dead_agents:\n",
    "            pygame.draw.circle(self.screen, (128, 128, 128),\n",
    "                               (int(dead_agent['x'] * self.cell_size + self.cell_size / 2),\n",
    "                                int(dead_agent['y'] * self.cell_size + self.cell_size / 2)),\n",
    "                               int(self.cell_size / 3))\n",
    "\n",
    "        for agent in self.agents:\n",
    "            pygame.draw.circle(self.screen, (255, 0, 0),\n",
    "                               (int(agent['x'] * self.cell_size + self.cell_size / 2),\n",
    "                                int(agent['y'] * self.cell_size + self.cell_size / 2)),\n",
    "                               int(self.cell_size / 3))\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def get_color(self, sugar_level):\n",
    "        if sugar_level == 0:\n",
    "            return (255, 255, 255)\n",
    "        else:\n",
    "            intensity = sugar_level / self.params['max_sugar']\n",
    "            return (255, 255, int(255 * (1 - intensity)))\n",
    "\n",
    "    def collect_data(self):\n",
    "        population = len(self.agents)\n",
    "        total_wealth = sum(agent['sugar'] for agent in self.agents)\n",
    "        average_wealth = total_wealth / population if population > 0 else 0\n",
    "\n",
    "        self.population_history.append(population)\n",
    "        self.average_wealth_history.append(average_wealth)\n",
    "        self.gini_coefficient_history.append(self.calculate_gini_coefficient())\n",
    "\n",
    "    def calculate_gini_coefficient(self):\n",
    "        if not self.agents:\n",
    "            return 0\n",
    "        wealth_values = sorted(agent['sugar'] for agent in self.agents)\n",
    "        cumulative_wealth = np.cumsum(wealth_values)\n",
    "        return (np.sum((2 * np.arange(1, len(wealth_values) + 1) - len(wealth_values) - 1) * wealth_values) /\n",
    "                (len(wealth_values) * np.sum(wealth_values)))\n",
    "\n",
    "    def plot_results(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.plot(self.population_history)\n",
    "        plt.title('Population over Time')\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Population')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.plot(self.average_wealth_history)\n",
    "        plt.title('Average Wealth over Time')\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Average Wealth')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.plot(self.gini_coefficient_history)\n",
    "        plt.title('Gini Coefficient over Time')\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Gini Coefficient')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def run_simulation(self, max_timesteps=1000):\n",
    "        running = True\n",
    "        while running and self.timestep < max_timesteps:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "\n",
    "            self.step()\n",
    "            self.render()\n",
    "            self.clock.tick(5)\n",
    "\n",
    "        self.plot_results()\n",
    "\n",
    "# Execution Parameters (same as training)\n",
    "params = {\n",
    "    'max_sugar': 5,\n",
    "    'growth_rate': 1,\n",
    "    'sugar_peak_frequency': 0.04,\n",
    "    'sugar_peak_spread': 6,\n",
    "    'job_center_duration': (40, 100),\n",
    "    'vision_range': 1,\n",
    "    'message_expiry': 15,\n",
    "    'max_relay_messages': 10,\n",
    "    'broadcast_radius': 15\n",
    "}\n",
    "\n",
    "# Create environment and run simulation\n",
    "env_visual = SugarscapeEnvironmentVisual(width=50, height=50, num_agents=1000, params=params)\n",
    "env_visual.run_simulation(max_timesteps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
