{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SugarscapeEnvironmentPPO' object has no attribute 'step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 494\u001b[0m\n\u001b[0;32m    491\u001b[0m MAX_TIMESTEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m    493\u001b[0m env \u001b[38;5;241m=\u001b[39m SugarscapeEnvironmentPPO(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, num_agents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed, num_episodes\u001b[38;5;241m=\u001b[39mTOTAL_EPISODES, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 494\u001b[0m env\u001b[38;5;241m.\u001b[39mrun_training(total_episodes\u001b[38;5;241m=\u001b[39mTOTAL_EPISODES, max_timesteps\u001b[38;5;241m=\u001b[39mMAX_TIMESTEPS)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[0;32m    497\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(env\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_agent_final.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 183\u001b[0m, in \u001b[0;36mSugarscapeEnvironmentPPO.run_training\u001b[1;34m(self, total_episodes, max_timesteps)\u001b[0m\n\u001b[0;32m    180\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Initialize total reward for the episode\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_timesteps):\n\u001b[1;32m--> 183\u001b[0m     timestep_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Modify step() to return total rewards\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timestep_reward\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Update policy every batch_size steps\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SugarscapeEnvironmentPPO' object has no attribute 'step'"
     ]
    }
   ],
   "source": [
    "# training_code_ppo.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.spatial import cKDTree\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\"\"\"\n",
    "Overview:\n",
    "\n",
    "PPO Agent architecture:\n",
    "input: agent x, agent y, agent sugar, message x, message y,\n",
    "relu activation\n",
    "\"\"\"\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "params = {\n",
    "            'max_sugar': 5,\n",
    "            'growth_rate': 1,\n",
    "            'sugar_peak_frequency': 0.05,\n",
    "            'sugar_peak_spread': 4,\n",
    "            'job_center_duration': (20, 50),\n",
    "            'vision_range': 1,\n",
    "            'message_expiry': 15,\n",
    "            'max_messages': 5,\n",
    "        }\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        # Common network layers\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Policy network\n",
    "        self.fc_policy = nn.Linear(128, 64)\n",
    "        self.relu_policy = nn.ReLU()\n",
    "        self.policy_head = nn.Linear(64, action_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Value network\n",
    "        self.fc_value = nn.Linear(128, 64)\n",
    "        self.relu_value = nn.ReLU()\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        for layer in [self.fc1, self.fc_policy, self.fc_value]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.xavier_uniform_(self.policy_head.weight)\n",
    "        nn.init.zeros_(self.policy_head.bias)\n",
    "        nn.init.xavier_uniform_(self.value_head.weight)\n",
    "        nn.init.zeros_(self.value_head.bias)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "\n",
    "        # Policy network forward\n",
    "        policy_x = self.relu_policy(self.fc_policy(x))\n",
    "        action_logits = self.policy_head(policy_x)\n",
    "\n",
    "        # Value network forward\n",
    "        value_x = self.relu_value(self.fc_value(x))\n",
    "        state_value = self.value_head(value_x)\n",
    "\n",
    "        return action_logits, state_value\n",
    "\n",
    "class SugarscapeEnvironmentPPO:\n",
    "    def __init__(self, width, height, num_agents, seed=None, num_episodes=200, params = None):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = seed\n",
    "\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        self.params = params\n",
    "        self.num_episodes = num_episodes\n",
    "        self.state_size = 5 + (2 * self.params['vision_range'] + 1) ** 2 + (3 * self.params['max_messages'])\n",
    "        self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.agent = PPOAgent(self.state_size, self.action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=1e-4)\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.2  # PPO clipping parameter\n",
    "        self.K_epochs = 4  # Number of epochs for updating policy\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # Storage for PPO\n",
    "        self.memory = []\n",
    "\n",
    "    def create_initial_sugar_peaks(self, num_peaks=2):\n",
    "        self.job_centers = []\n",
    "        for _ in range(num_peaks):\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "    def create_job_center(self):\n",
    "        x, y = np.random.randint(0, self.width), np.random.randint(0, self.height)\n",
    "        duration = np.random.randint(*self.params['job_center_duration'])\n",
    "        self.job_centers.append({\n",
    "            'x': x, 'y': y,\n",
    "            'duration': duration,\n",
    "            'max_sugar': self.params['max_sugar']\n",
    "        })\n",
    "\n",
    "    def update_sugar_landscape(self):\n",
    "        self.sugar = np.zeros((self.height, self.width))\n",
    "        for center in self.job_centers:\n",
    "            x_grid, y_grid = np.meshgrid(np.arange(self.width), np.arange(self.height))\n",
    "            distance = np.sqrt((x_grid - center['x']) ** 2 + (y_grid - center['y']) ** 2)\n",
    "            sugar_level = center['max_sugar'] * np.exp(-distance ** 2 / (2 * self.params['sugar_peak_spread'] ** 2))\n",
    "            self.sugar += sugar_level\n",
    "        self.sugar = np.clip(self.sugar, 0, self.params['max_sugar'])\n",
    "        self.sugar = np.round(self.sugar).astype(int)\n",
    "\n",
    "    def initialize_agents(self):\n",
    "        agents = []\n",
    "        available_positions = set((x, y) for x in range(self.width) for y in range(self.height))\n",
    "        for i in range(self.num_agents):\n",
    "            if not available_positions:\n",
    "                break\n",
    "            x, y = available_positions.pop()\n",
    "            agents.append(self.create_agent(i, x, y))\n",
    "        return agents\n",
    "\n",
    "    def create_agent(self, id, x, y):\n",
    "        return {\n",
    "            'id': id,\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'sugar': np.random.randint(20, 50),\n",
    "            'metabolism': np.random.randint(1, 3),\n",
    "            'vision': self.params['vision_range'],\n",
    "            'messages': deque(maxlen=self.params['max_messages']),\n",
    "            'destination': None,\n",
    "            'memory': [],\n",
    "            'path': [],\n",
    "            'age': 0\n",
    "        }\n",
    "\n",
    "    def reset_environment(self):\n",
    "        self.timestep = 0\n",
    "        self.job_centers = []\n",
    "        self.sugar = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.create_initial_sugar_peaks()\n",
    "        self.agents = self.initialize_agents()\n",
    "        self.agent_positions = set((agent['x'], agent['y']) for agent in self.agents)\n",
    "        self.dead_agents = []\n",
    "        self.memory = []\n",
    "\n",
    "    def run_training(self, total_episodes=None, max_timesteps=1000):\n",
    "        # Create checkpoints directory if it doesn't exist\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "        # Open CSV file for writing\n",
    "        with open('training_log.csv', mode='w', newline='') as csv_file:\n",
    "            fieldnames = ['Episode', 'TotalReward', 'FinalAgentPopulation']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            if total_episodes is None:\n",
    "                total_episodes = self.num_episodes\n",
    "\n",
    "            for episode in range(total_episodes):\n",
    "                self.reset_environment()\n",
    "                total_reward = 0  # Initialize total reward for the episode\n",
    "\n",
    "                for t in range(max_timesteps):\n",
    "                    timestep_reward = self.step()  # Modify step() to return total rewards\n",
    "                    total_reward += timestep_reward\n",
    "\n",
    "                    # Update policy every batch_size steps\n",
    "                    if len(self.memory) >= self.batch_size:\n",
    "                        self.train_ppo()\n",
    "                        self.memory = []\n",
    "\n",
    "                    self.timestep += 1  # Increment timestep here\n",
    "\n",
    "                # Log data for the episode\n",
    "                final_agent_population = len(self.agents)\n",
    "                log_entry = {\n",
    "                    'Episode': episode + 1,\n",
    "                    'TotalReward': total_reward,\n",
    "                    'FinalAgentPopulation': final_agent_population\n",
    "                }\n",
    "                writer.writerow(log_entry)\n",
    "\n",
    "                print(f\"Episode {episode + 1} completed. Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "                if (episode + 1) % 10 == 0:\n",
    "                    print(f\"Completed Episode: {episode + 1}\")\n",
    "                    # Save model every 100 episodes into checkpoints folder\n",
    "                    torch.save(self.agent.state_dict(), f'checkpoints/ppo_agent_episode_{episode + 1}.pth')\n",
    "\n",
    "        print(\"\\nTraining completed and log saved to 'training_log.csv'.\")\n",
    "\n",
    "    def get_state(self, agent):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        sugar = agent['sugar'] / 100  # Normalize sugar level\n",
    "        metabolism = agent['metabolism'] / 5  # Normalize metabolism\n",
    "        vision = agent['vision'] / 5  # Normalize vision\n",
    "\n",
    "        # Extract sugar levels within vision range\n",
    "        vision_range = agent['vision']\n",
    "        y_min = max(0, y - vision_range)\n",
    "        y_max = min(self.height, y + vision_range + 1)\n",
    "        x_min = max(0, x - vision_range)\n",
    "        x_max = min(self.width, x + vision_range + 1)\n",
    "        sugar_map = self.sugar[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # Pad the sugar map to a fixed size\n",
    "        expected_size = (2 * vision_range + 1, 2 * vision_range + 1)\n",
    "        pad_h = expected_size[0] - sugar_map.shape[0]\n",
    "        pad_w = expected_size[1] - sugar_map.shape[1]\n",
    "\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        padded_sugar_map = np.pad(sugar_map, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "        sugar_map_flat = padded_sugar_map.flatten() / self.params['max_sugar']\n",
    "\n",
    "        # Encode messages\n",
    "        N = self.params['max_messages']\n",
    "        messages = list(agent['messages'])[-N:]\n",
    "        message_features = []\n",
    "        for msg in messages:\n",
    "            # Normalize message coordinates relative to grid size\n",
    "            msg_x = msg['x'] / self.width\n",
    "            msg_y = msg['y'] / self.height\n",
    "            msg_sugar = msg['sugar_amount'] / self.params['max_sugar']\n",
    "            message_features.extend([msg_x, msg_y, msg_sugar])\n",
    "        # Pad remaining messages with zeros if fewer than N\n",
    "        while len(message_features) < 3 * N:\n",
    "            message_features.extend([0.0, 0.0, 0.0])\n",
    "\n",
    "        state = np.concatenate(([x / self.width, y / self.height, sugar, metabolism, vision], sugar_map_flat, message_features))\n",
    "        return state\n",
    "\n",
    "    def select_action(self, state, valid_actions):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_logits, _ = self.agent(state_tensor)\n",
    "        action_probs = torch.softmax(action_logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "        # Check for NaNs\n",
    "        if np.isnan(action_probs).any():\n",
    "            print(\"Error: action_probs contains NaN values.\")\n",
    "            action_probs = np.ones(self.action_size) / self.action_size\n",
    "\n",
    "    # Mask invalid actions\n",
    "    mask = np.zeros(self.action_size, dtype=bool)\n",
    "    mask[valid_actions] = True\n",
    "    masked_probs = action_probs * mask\n",
    "\n",
    "    total_prob = masked_probs.sum()\n",
    "    if total_prob == 0:\n",
    "        print(\"Warning: Sum of masked_probs is zero. Assigning equal probabilities to valid actions.\")\n",
    "        masked_probs = mask.astype(float)\n",
    "        masked_probs /= masked_probs.sum()\n",
    "    else:\n",
    "        masked_probs /= total_prob\n",
    "\n",
    "    action = np.random.choice(self.action_size, p=masked_probs)\n",
    "    return action, action_probs\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, agent):\n",
    "        actions = []\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, y - 1),  # Up\n",
    "            1: (x, y + 1),  # Down\n",
    "            2: (x - 1, y),  # Left\n",
    "            3: (x + 1, y),  # Right\n",
    "            4: (x, y)       # Stay\n",
    "        }\n",
    "        for action, (nx, ny) in possible_moves.items():\n",
    "            if 0 <= nx < self.width and 0 <= ny < self.height:\n",
    "                if (nx, ny) not in self.agent_positions or (nx, ny) == (x, y):\n",
    "                    actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, y - 1),  # Up\n",
    "            1: (x, y + 1),  # Down\n",
    "            2: (x - 1, y),  # Left\n",
    "            3: (x + 1, y),  # Right\n",
    "            4: (x, y)       # Stay\n",
    "        }\n",
    "        nx, ny = possible_moves[action]\n",
    "        if (0 <= nx < self.width and 0 <= ny < self.height and\n",
    "                ((nx, ny) not in self.agent_positions or (nx, ny) == (x, y))):\n",
    "            self.agent_positions.remove((x, y))\n",
    "            agent['x'], agent['y'] = nx, ny\n",
    "            agent['path'].append((agent['x'], agent['y']))\n",
    "            self.agent_positions.add((nx, ny))\n",
    "\n",
    "    def collect_sugar_and_update_agent(self, agent):\n",
    "        collected_sugar = self.sugar[agent['y'], agent['x']]\n",
    "        agent['sugar'] += collected_sugar\n",
    "        self.sugar[agent['y'], agent['x']] = 0\n",
    "        agent['sugar'] -= agent['metabolism']\n",
    "        agent['age'] += 1\n",
    "\n",
    "    def broadcast_messages(self):\n",
    "        if not self.agents:\n",
    "            return  # No agents to broadcast\n",
    "\n",
    "        positions = np.array([[agent['x'], agent['y']] for agent in self.agents])\n",
    "        tree = cKDTree(positions)\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            # Identify visible sugar peaks\n",
    "            visible_sugar = self.get_visible_sugar(agent)\n",
    "            sugar_locations = np.argwhere(visible_sugar > 0)\n",
    "            messages = []\n",
    "            for loc in sugar_locations:\n",
    "                msg_x = agent['x'] + loc[1] - agent['vision']\n",
    "                msg_y = agent['y'] + loc[0] - agent['vision']\n",
    "                # Ensure message coordinates are within grid\n",
    "                msg_x = int(np.clip(msg_x, 0, self.width - 1))\n",
    "                msg_y = int(np.clip(msg_y, 0, self.height - 1))\n",
    "                msg = {\n",
    "                    'sender_id': agent['id'],\n",
    "                    'timestep': self.timestep,\n",
    "                    'sugar_amount': self.sugar[msg_y, msg_x],\n",
    "                    'x': msg_x,\n",
    "                    'y': msg_y\n",
    "                }\n",
    "                messages.append(msg)\n",
    "\n",
    "            # Broadcast to neighbors within broadcast_radius\n",
    "            radius = 5  # Fixed broadcast radius\n",
    "            neighbors = tree.query_ball_point([agent['x'], agent['y']], radius)\n",
    "            for neighbor_idx in neighbors:\n",
    "                if neighbor_idx != i:\n",
    "                    for msg in messages:\n",
    "                        self.agents[neighbor_idx]['messages'].append(msg)\n",
    "\n",
    "    def get_visible_sugar(self, agent):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        vision = agent['vision']\n",
    "        y_min = max(0, y - vision)\n",
    "        y_max = min(self.height, y + vision + 1)\n",
    "        x_min = max(0, x - vision)\n",
    "        x_max = min(self.width, x + vision + 1)\n",
    "        visible_area = self.sugar[y_min:y_max, x_min:x_max]\n",
    "        return visible_area\n",
    "\n",
    "    def step(self):\n",
    "        # Update job centers and sugar landscape\n",
    "        for center in self.job_centers:\n",
    "            center['duration'] -= 1\n",
    "        self.job_centers = [center for center in self.job_centers if center['duration'] > 0]\n",
    "        if np.random.random() < self.params['sugar_peak_frequency']:\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "        # Broadcast messages\n",
    "        self.broadcast_messages()\n",
    "\n",
    "        total_rewards = 0  # Initialize total rewards for this timestep\n",
    "\n",
    "        # For each agent, select action and collect experience\n",
    "        for agent in self.agents:\n",
    "            state = self.get_state(agent)\n",
    "            valid_actions = self.get_valid_actions(agent)\n",
    "            if not valid_actions:\n",
    "                continue  # Skip if no valid actions\n",
    "            action, action_probs = self.select_action(state, valid_actions)\n",
    "            prev_sugar = agent['sugar']\n",
    "            self.move_agent(agent, action)\n",
    "            self.collect_sugar_and_update_agent(agent)\n",
    "            next_state = self.get_state(agent)\n",
    "            reward = (agent['sugar'] - prev_sugar) / 10.0  # Normalize reward\n",
    "            total_rewards += reward  # Accumulate total rewards\n",
    "            done = agent['sugar'] <= 0\n",
    "\n",
    "            # Store experience\n",
    "            agent['memory'].append({\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'action_prob': action_probs[action],\n",
    "                'reward': reward,\n",
    "                'next_state': next_state,\n",
    "                'done': done\n",
    "            })\n",
    "\n",
    "            # Add to global memory\n",
    "            self.memory.append(agent['memory'][-1])\n",
    "\n",
    "        # Handle agent death\n",
    "        alive_agents = []\n",
    "        for agent in self.agents:\n",
    "            if agent['sugar'] <= 0:\n",
    "                self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            else:\n",
    "                alive_agents.append(agent)\n",
    "        self.agents = alive_agents\n",
    "\n",
    "        # Replenish agents\n",
    "        self.replenish_agents()\n",
    "\n",
    "        return total_rewards  # Return total rewards collected in this timestep\n",
    "\n",
    "    def train_ppo(self):\n",
    "        # Convert memory to tensors\n",
    "        states = torch.FloatTensor(np.array([m['state'] for m in self.memory])).to(device)\n",
    "        actions = torch.LongTensor(np.array([m['action'] for m in self.memory])).unsqueeze(1).to(device)\n",
    "        old_action_probs = torch.FloatTensor(np.array([m['action_prob'] for m in self.memory])).unsqueeze(1).to(device)\n",
    "        rewards = [m['reward'] for m in self.memory]\n",
    "        dones = [m['done'] for m in self.memory]\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            if done:\n",
    "                R = 0\n",
    "            R = reward + self.gamma * R\n",
    "            discounted_rewards.insert(0, R)\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards).unsqueeze(1).to(device)\n",
    "\n",
    "        # Normalize rewards\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Get action probabilities and state values\n",
    "            action_probs, state_values = self.agent(states)\n",
    "            action_probs = action_probs.gather(1, actions)\n",
    "            state_values = state_values\n",
    "\n",
    "            # Calculate ratios\n",
    "            ratios = action_probs / old_action_probs\n",
    "\n",
    "            # Calculate advantages\n",
    "            advantages = discounted_rewards - state_values.detach()\n",
    "\n",
    "            # Compute surrogate losses\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            # Policy loss\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value_loss = nn.MSELoss()(state_values, discounted_rewards)\n",
    "\n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "            # Optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    def replenish_agents(self):\n",
    "        while len(self.agents) < self.num_agents:\n",
    "            x, y = random.randint(0, self.width - 1), random.randint(0, self.height - 1)\n",
    "            if (x, y) not in self.agent_positions:\n",
    "                agent_id = max([agent['id'] for agent in self.agents] + [0]) + 1\n",
    "                new_agent = self.create_agent(agent_id, x, y)\n",
    "                self.agent_positions.add((x, y))\n",
    "                self.agents.append(new_agent)\n",
    "\n",
    "# Initialize and run training\n",
    "if __name__ == \"__main__\":\n",
    "    seed = 42  # Set a seed for reproducibility\n",
    "    TOTAL_EPISODES = 600\n",
    "    MAX_TIMESTEPS = 500\n",
    "\n",
    "    env = SugarscapeEnvironmentPPO(width=15, height=15, num_agents=100, seed=seed, num_episodes=TOTAL_EPISODES, params=params)\n",
    "    env.run_training(total_episodes=TOTAL_EPISODES, max_timesteps=MAX_TIMESTEPS)\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(env.agent.state_dict(), 'ppo_agent_final.pth')\n",
    "\n",
    "    print(\"\\nTraining completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/ppo_agent_episode_10.pth\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# simulation_with_pygame_ppo.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.spatial import cKDTree\n",
    "import re\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import QUIT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set up device (ensure this matches your training code)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the PPOAgent class\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        # Common network layers\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Policy network\n",
    "        self.fc_policy = nn.Linear(128, 64)\n",
    "        self.relu_policy = nn.ReLU()\n",
    "        self.policy_head = nn.Linear(64, action_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Value network\n",
    "        self.fc_value = nn.Linear(128, 64)\n",
    "        self.relu_value = nn.ReLU()\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        for layer in [self.fc1, self.fc_policy, self.fc_value]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.xavier_uniform_(self.policy_head.weight)\n",
    "        nn.init.zeros_(self.policy_head.bias)\n",
    "        nn.init.xavier_uniform_(self.value_head.weight)\n",
    "        nn.init.zeros_(self.value_head.bias)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "\n",
    "        # Policy network forward\n",
    "        policy_x = self.relu_policy(self.fc_policy(x))\n",
    "        action_logits = self.policy_head(policy_x)\n",
    "        action_probs = self.softmax(action_logits)\n",
    "\n",
    "        # Value network forward\n",
    "        value_x = self.relu_value(self.fc_value(x))\n",
    "        state_value = self.value_head(value_x)\n",
    "\n",
    "        return action_probs, state_value\n",
    "\n",
    "# Define the SugarscapeEnvironmentPPO class\n",
    "class SugarscapeEnvironmentPPO:\n",
    "    def __init__(self, width, height, num_agents, seed=None, params = None):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = seed\n",
    "\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "        self.state_size = 5 + (2 * self.params['vision_range'] + 1) ** 2 + (3 * self.params['max_messages'])\n",
    "        self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.agent_model = PPOAgent(self.state_size, self.action_size).to(device)\n",
    "\n",
    "        # Load the trained model\n",
    "        model_path = self.get_latest_model_path()\n",
    "        if model_path:\n",
    "            print(f\"Loading model from {model_path}\")\n",
    "            self.agent_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            self.agent_model.eval()\n",
    "            self.agent_model.to(device)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No trained model found. Please train the model first.\")\n",
    "\n",
    "    def create_initial_sugar_peaks(self, num_peaks=2):\n",
    "        self.job_centers = []\n",
    "        for _ in range(num_peaks):\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "    def create_job_center(self):\n",
    "        x, y = np.random.randint(0, self.width), np.random.randint(0, self.height)\n",
    "        duration = np.random.randint(*self.params['job_center_duration'])\n",
    "        self.job_centers.append({\n",
    "            'x': x, 'y': y,\n",
    "            'duration': duration,\n",
    "            'max_sugar': self.params['max_sugar']\n",
    "        })\n",
    "\n",
    "    def update_sugar_landscape(self):\n",
    "        self.sugar = np.zeros((self.height, self.width))\n",
    "        for center in self.job_centers:\n",
    "            x_grid, y_grid = np.meshgrid(np.arange(self.width), np.arange(self.height))\n",
    "            distance = np.sqrt((x_grid - center['x']) ** 2 + (y_grid - center['y']) ** 2)\n",
    "            sugar_level = center['max_sugar'] * np.exp(-distance ** 2 / (2 * self.params['sugar_peak_spread'] ** 2))\n",
    "            self.sugar += sugar_level\n",
    "        self.sugar = np.clip(self.sugar, 0, self.params['max_sugar'])\n",
    "        self.sugar = np.round(self.sugar).astype(int)\n",
    "\n",
    "    def initialize_agents(self):\n",
    "        agents = []\n",
    "        available_positions = set((x, y) for x in range(self.width) for y in range(self.height))\n",
    "        for i in range(self.num_agents):\n",
    "            if not available_positions:\n",
    "                break\n",
    "            x, y = available_positions.pop()\n",
    "            agents.append(self.create_agent(i, x, y))\n",
    "        return agents\n",
    "\n",
    "    def create_agent(self, id, x, y):\n",
    "        return {\n",
    "            'id': id,\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'sugar': np.random.randint(20, 50),\n",
    "            'metabolism': np.random.randint(1, 3),\n",
    "            'vision': self.params['vision_range'],\n",
    "            'messages': deque(maxlen=self.params['max_messages']),\n",
    "            'destination': None,\n",
    "            'memory': [],\n",
    "            'path': [],\n",
    "            'age': 0\n",
    "        }\n",
    "\n",
    "    def reset_environment(self):\n",
    "        self.timestep = 0\n",
    "        self.job_centers = []\n",
    "        self.sugar = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.create_initial_sugar_peaks()\n",
    "        self.agents = self.initialize_agents()\n",
    "        self.agent_positions = set((agent['x'], agent['y']) for agent in self.agents)\n",
    "        self.dead_agents = []\n",
    "\n",
    "    def get_latest_model_path(self):\n",
    "        # Check for final model\n",
    "        final_model = 'ppo_agent_final.pth'\n",
    "        if os.path.exists(final_model):\n",
    "            return final_model\n",
    "        # If final model doesn't exist, find the latest checkpoint\n",
    "        checkpoint_pattern = r'ppo_agent_episode_(\\d+)\\.pth'\n",
    "        checkpoints = [f for f in os.listdir('checkpoints') if re.match(checkpoint_pattern, f)]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "        # Extract episode numbers and find the latest\n",
    "        episodes = [int(re.findall(checkpoint_pattern, f)[0]) for f in checkpoints]\n",
    "        latest_episode = max(episodes)\n",
    "        latest_checkpoint = f'checkpoints/ppo_agent_episode_{latest_episode}.pth'\n",
    "        return latest_checkpoint\n",
    "\n",
    "    def run_simulation(self, max_timesteps=100):\n",
    "        self.reset_environment()\n",
    "        # Pygame initialization\n",
    "        pygame.init()\n",
    "        self.cell_size = 20  # Size of each grid cell in pixels\n",
    "        self.screen_width = self.width * self.cell_size\n",
    "        self.screen_height = self.height * self.cell_size\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption(\"Sugarscape Simulation with PPO\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            self.step_simulation()\n",
    "            self.draw_environment()\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(10)  # Adjust the speed as needed\n",
    "\n",
    "            # Handle Pygame events\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "    def step_simulation(self):\n",
    "        # Update job centers and sugar landscape\n",
    "        for center in self.job_centers:\n",
    "            center['duration'] -= 1\n",
    "        self.job_centers = [center for center in self.job_centers if center['duration'] > 0]\n",
    "        if np.random.random() < self.params['sugar_peak_frequency']:\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "        # Broadcast messages\n",
    "        self.broadcast_messages()\n",
    "\n",
    "        # For each agent, select action\n",
    "        for agent in self.agents:\n",
    "            state = self.get_state(agent)\n",
    "            valid_actions = self.get_valid_actions(agent)\n",
    "            if not valid_actions:\n",
    "                continue  # Skip if no valid actions\n",
    "            action = self.select_action(state, valid_actions)\n",
    "            self.move_agent(agent, action)\n",
    "            self.collect_sugar_and_update_agent(agent)\n",
    "\n",
    "        # Handle agent death\n",
    "        alive_agents = []\n",
    "        for agent in self.agents:\n",
    "            if agent['sugar'] <= 0:\n",
    "                self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            else:\n",
    "                alive_agents.append(agent)\n",
    "        self.agents = alive_agents\n",
    "\n",
    "        # Replenish agents\n",
    "        self.replenish_agents_simulation()\n",
    "\n",
    "    def replenish_agents_simulation(self):\n",
    "        # Do not replenish agents during simulation to observe agent behavior\n",
    "        pass\n",
    "\n",
    "    def draw_environment(self):\n",
    "        # Clear the screen\n",
    "        self.screen.fill((255, 255, 255))  # White background\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                sugar_amount = self.sugar[y, x]\n",
    "                if sugar_amount > 0:\n",
    "                    color_intensity = int(255 * sugar_amount / self.params['max_sugar'])\n",
    "                    color = (255, 255 - color_intensity, 255 - color_intensity)\n",
    "                    rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "                    pygame.draw.rect(self.screen, color, rect)\n",
    "\n",
    "        # Draw agents\n",
    "        for agent in self.agents:\n",
    "            x, y = agent['x'], agent['y']\n",
    "            rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (0, 0, 255), rect)  # Blue color for agents\n",
    "\n",
    "    # Rest of the methods (get_state, select_action, move_agent, etc.) are similar to training code\n",
    "\n",
    "    def get_state(self, agent):\n",
    "        # Same as in training code\n",
    "        x, y = agent['x'], agent['y']\n",
    "        sugar = agent['sugar'] / 100  # Normalize sugar level\n",
    "        metabolism = agent['metabolism'] / 5  # Normalize metabolism\n",
    "        vision = agent['vision'] / 5  # Normalize vision\n",
    "\n",
    "        # Extract sugar levels within vision range\n",
    "        vision_range = agent['vision']\n",
    "        y_min = max(0, y - vision_range)\n",
    "        y_max = min(self.height, y + vision_range + 1)\n",
    "        x_min = max(0, x - vision_range)\n",
    "        x_max = min(self.width, x + vision_range + 1)\n",
    "        sugar_map = self.sugar[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # Pad the sugar map to a fixed size\n",
    "        expected_size = (2 * vision_range + 1, 2 * vision_range + 1)\n",
    "        pad_h = expected_size[0] - sugar_map.shape[0]\n",
    "        pad_w = expected_size[1] - sugar_map.shape[1]\n",
    "\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        padded_sugar_map = np.pad(sugar_map, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "        sugar_map_flat = padded_sugar_map.flatten() / self.params['max_sugar']\n",
    "\n",
    "        # Encode messages\n",
    "        N = self.params['max_messages']\n",
    "        messages = list(agent['messages'])[-N:]\n",
    "        message_features = []\n",
    "        for msg in messages:\n",
    "            # Normalize message coordinates relative to grid size\n",
    "            msg_x = msg['x'] / self.width\n",
    "            msg_y = msg['y'] / self.height\n",
    "            msg_sugar = msg['sugar_amount'] / self.params['max_sugar']\n",
    "            message_features.extend([msg_x, msg_y, msg_sugar])\n",
    "        # Pad remaining messages with zeros if fewer than N\n",
    "        while len(message_features) < 3 * N:\n",
    "            message_features.extend([0.0, 0.0, 0.0])\n",
    "\n",
    "        state = np.concatenate(([x / self.width, y / self.height, sugar, metabolism, vision], sugar_map_flat, message_features))\n",
    "        return state\n",
    "\n",
    "    def select_action(self, state, valid_actions):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_probs, _ = self.agent_model(state_tensor)\n",
    "        action_probs = action_probs.squeeze().cpu().numpy()\n",
    "\n",
    "        # Mask invalid actions\n",
    "        mask = np.zeros(self.action_size, dtype=bool)\n",
    "        mask[valid_actions] = True\n",
    "        masked_probs = action_probs * mask\n",
    "        masked_probs /= masked_probs.sum()\n",
    "\n",
    "        action = np.random.choice(self.action_size, p=masked_probs)\n",
    "        return action\n",
    "\n",
    "    def get_valid_actions(self, agent):\n",
    "        # Same as in training code\n",
    "        actions = []\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, y - 1),  # Up\n",
    "            1: (x, y + 1),  # Down\n",
    "            2: (x - 1, y),  # Left\n",
    "            3: (x + 1, y),  # Right\n",
    "            4: (x, y)       # Stay\n",
    "        }\n",
    "        for action, (nx, ny) in possible_moves.items():\n",
    "            if 0 <= nx < self.width and 0 <= ny < self.height:\n",
    "                if (nx, ny) not in self.agent_positions or (nx, ny) == (x, y):\n",
    "                    actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        # Same as in training code\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, y - 1),  # Up\n",
    "            1: (x, y + 1),  # Down\n",
    "            2: (x - 1, y),  # Left\n",
    "            3: (x + 1, y),  # Right\n",
    "            4: (x, y)       # Stay\n",
    "        }\n",
    "        nx, ny = possible_moves[action]\n",
    "        if (0 <= nx < self.width and 0 <= ny < self.height and\n",
    "                ((nx, ny) not in self.agent_positions or (nx, ny) == (x, y))):\n",
    "            self.agent_positions.remove((x, y))\n",
    "            agent['x'], agent['y'] = nx, ny\n",
    "            agent['path'].append((agent['x'], agent['y']))\n",
    "            self.agent_positions.add((nx, ny))\n",
    "\n",
    "    def collect_sugar_and_update_agent(self, agent):\n",
    "        # Same as in training code\n",
    "        collected_sugar = self.sugar[agent['y'], agent['x']]\n",
    "        agent['sugar'] += collected_sugar\n",
    "        self.sugar[agent['y'], agent['x']] = 0\n",
    "        agent['sugar'] -= agent['metabolism']\n",
    "        agent['age'] += 1\n",
    "\n",
    "    def broadcast_messages(self):\n",
    "        # Same as in training code\n",
    "        if not self.agents:\n",
    "            return  # No agents to broadcast\n",
    "\n",
    "        positions = np.array([[agent['x'], agent['y']] for agent in self.agents])\n",
    "        tree = cKDTree(positions)\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            # Identify visible sugar peaks\n",
    "            visible_sugar = self.get_visible_sugar(agent)\n",
    "            sugar_locations = np.argwhere(visible_sugar > 0)\n",
    "            messages = []\n",
    "            for loc in sugar_locations:\n",
    "                msg_x = agent['x'] + loc[1] - agent['vision']\n",
    "                msg_y = agent['y'] + loc[0] - agent['vision']\n",
    "                # Ensure message coordinates are within grid\n",
    "                msg_x = int(np.clip(msg_x, 0, self.width - 1))\n",
    "                msg_y = int(np.clip(msg_y, 0, self.height - 1))\n",
    "                msg = {\n",
    "                    'sender_id': agent['id'],\n",
    "                    'timestep': self.timestep,\n",
    "                    'sugar_amount': self.sugar[msg_y, msg_x],\n",
    "                    'x': msg_x,\n",
    "                    'y': msg_y\n",
    "                }\n",
    "                messages.append(msg)\n",
    "\n",
    "            # Broadcast to neighbors within broadcast_radius\n",
    "            radius = 5  # Fixed broadcast radius\n",
    "            neighbors = tree.query_ball_point([agent['x'], agent['y']], radius)\n",
    "            for neighbor_idx in neighbors:\n",
    "                if neighbor_idx != i:\n",
    "                    for msg in messages:\n",
    "                        self.agents[neighbor_idx]['messages'].append(msg)\n",
    "\n",
    "    def get_visible_sugar(self, agent):\n",
    "        # Same as in training code\n",
    "        x, y = agent['x'], agent['y']\n",
    "        vision = agent['vision']\n",
    "        y_min = max(0, y - vision)\n",
    "        y_max = min(self.height, y + vision + 1)\n",
    "        x_min = max(0, x - vision)\n",
    "        x_max = min(self.width, x + vision + 1)\n",
    "        visible_area = self.sugar[y_min:y_max, x_min:x_max]\n",
    "        return visible_area\n",
    "\n",
    "# Run the simulation\n",
    "if __name__ == \"__main__\":\n",
    "    seed = 42  # Set a seed for reproducibility\n",
    "    try:\n",
    "        env = SugarscapeEnvironmentPPO(width=30, height=30, num_agents=400, seed=seed, params=params)\n",
    "        MAX_TIMESTEPS = 1000  # Set the number of timesteps for the simulation\n",
    "        env.run_simulation(max_timesteps=MAX_TIMESTEPS)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
