{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neo\\AppData\\Local\\Temp\\ipykernel_42752\\1676825964.py:86: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  observations = torch.tensor(self.memory['observations'], dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 406\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Train the PPO agent without visualizing the environment\u001b[39;00m\n\u001b[0;32m    404\u001b[0m env \u001b[38;5;241m=\u001b[39m SugarscapeEnvironment(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_agents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, cell_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    405\u001b[0m                             broadcast_radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, show_sugar_levels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, show_broadcast_radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, show_agent_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 406\u001b[0m env\u001b[38;5;241m.\u001b[39mtrain(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Run the final simulation with rendering\u001b[39;00m\n\u001b[0;32m    409\u001b[0m env\u001b[38;5;241m.\u001b[39mfinal_simulation(max_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 290\u001b[0m, in \u001b[0;36mSugarscapeEnvironment.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    288\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    292\u001b[0m         elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[1], line 282\u001b[0m, in \u001b[0;36mSugarscapeEnvironment.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdead_agents \u001b[38;5;241m=\u001b[39m [agent \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdead_agents \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m-\u001b[39m agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeath_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mmemory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mupdate_policy()\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_data()\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m, in \u001b[0;36mPPOAgent.update_policy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m     loss \u001b[38;5;241m=\u001b[39m actor_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m critic_loss\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 105\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Clear memory after update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Neo\\anaconda3\\envs\\ACML\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Neo\\anaconda3\\envs\\ACML\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Neo\\anaconda3\\envs\\ACML\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "\n",
    "# Define the Policy Network\n",
    "class SharedPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(SharedPolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.action_head = nn.Linear(128, action_size)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action_logits = self.action_head(x)\n",
    "        state_value = self.value_head(x)\n",
    "        return action_logits, state_value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_size, action_size, lr=3e-4, gamma=0.99, epsilon=0.2, lambda_=0.95, ppo_epochs=4, batch_size=64):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_ = lambda_\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.policy_net = SharedPolicyNetwork(input_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "\n",
    "        # Memory for PPO\n",
    "        self.memory = {\n",
    "            'observations': [],\n",
    "            'actions': [],\n",
    "            'log_probs': [],\n",
    "            'rewards': [],\n",
    "            'dones': [],\n",
    "            'values': []\n",
    "        }\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)\n",
    "        action_logits, state_value = self.policy_net(observation)\n",
    "        action_probs = torch.softmax(action_logits, dim=-1)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob, state_value\n",
    "\n",
    "    def store_experience(self, observation, action, log_prob, reward, done, value):\n",
    "        self.memory['observations'].append(observation)\n",
    "        self.memory['actions'].append(action)\n",
    "        self.memory['log_probs'].append(log_prob)\n",
    "        self.memory['rewards'].append(reward)\n",
    "        self.memory['dones'].append(done)\n",
    "        self.memory['values'].append(value)\n",
    "\n",
    "    def compute_gae(self, rewards, dones, values):\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        next_value = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            mask = 1 - dones[i]\n",
    "            delta = rewards[i] + self.gamma * next_value * mask - values[i]\n",
    "            gae = delta + self.gamma * self.lambda_ * gae * mask\n",
    "            advantages.insert(0, gae)\n",
    "            next_value = values[i]\n",
    "            returns.insert(0, gae + values[i])\n",
    "        return torch.tensor(returns), torch.tensor(advantages)\n",
    "\n",
    "    def update_policy(self):\n",
    "        rewards = self.memory['rewards']\n",
    "        dones = self.memory['dones']\n",
    "        values = self.memory['values']\n",
    "        log_probs_old = torch.stack(self.memory['log_probs']).detach()\n",
    "        observations = torch.tensor(self.memory['observations'], dtype=torch.float32)\n",
    "        actions = torch.tensor(self.memory['actions'])\n",
    "        returns, advantages = self.compute_gae(rewards, dones, values)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            action_logits, state_values = self.policy_net(observations)\n",
    "            action_probs = torch.softmax(action_logits, dim=-1)\n",
    "            dist = Categorical(action_probs)\n",
    "            log_probs_new = dist.log_prob(actions)\n",
    "            ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Clear memory after update\n",
    "        for key in self.memory.keys():\n",
    "            self.memory[key] = []\n",
    "\n",
    "class SugarscapeEnvironment:\n",
    "    def __init__(self, width, height, num_agents, cell_size=10, show_sugar_levels=True,\n",
    "                 show_broadcast_radius=True, show_agent_paths=True, broadcast_radius=5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_agents = num_agents\n",
    "        self.cell_size = cell_size\n",
    "        self.show_sugar_levels = show_sugar_levels\n",
    "        self.show_broadcast_radius = show_broadcast_radius\n",
    "        self.show_agent_paths = show_agent_paths\n",
    "        self.broadcast_radius = broadcast_radius\n",
    "\n",
    "        self.params = {\n",
    "            'max_sugar': 5,\n",
    "            'growth_rate': 1,\n",
    "            'sugar_peak_frequency': 0.04,\n",
    "            'sugar_peak_spread': 6,\n",
    "            'job_center_duration': (40, 100),\n",
    "            'vision_range': 1,\n",
    "            'message_expiry': 15,\n",
    "            'max_relay_messages': 10,\n",
    "            'gamma': 0.99,\n",
    "            'lambda': 0.95,\n",
    "            'epsilon': 0.2,\n",
    "            'ppo_epochs': 4,\n",
    "            'batch_size': 64,\n",
    "            'learning_rate': 3e-4,\n",
    "        }\n",
    "\n",
    "        self.job_centers = []\n",
    "        self.sugar = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.create_initial_sugar_peaks()\n",
    "        self.max_sugar_landscape = self.sugar.copy()\n",
    "        self.agents = self.initialize_agents()\n",
    "        self.agent_positions = set((agent['x'], agent['y']) for agent in self.agents)\n",
    "        self.dead_agents = []\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((width * cell_size, height * cell_size))\n",
    "        pygame.display.set_caption(\"Sugarscape Simulation - With PPO\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.font = pygame.font.Font(None, 10)\n",
    "\n",
    "        self.population_history = []\n",
    "        self.average_wealth_history = []\n",
    "        self.gini_coefficient_history = []\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Initialize PPO agent\n",
    "        self.observation_space_size = (self.params['vision_range'] * 2 + 1) ** 2 + 2\n",
    "        self.action_space_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.agent = PPOAgent(self.observation_space_size, self.action_space_size)\n",
    "\n",
    "    def create_initial_sugar_peaks(self, num_peaks=2):\n",
    "        for _ in range(num_peaks):\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "    def create_job_center(self):\n",
    "        x, y = np.random.randint(0, self.width), np.random.randint(0, self.height)\n",
    "        duration = np.random.randint(*self.params['job_center_duration'])\n",
    "        self.job_centers.append({\n",
    "            'x': x, 'y': y,\n",
    "            'duration': duration,\n",
    "            'max_sugar': self.params['max_sugar']\n",
    "        })\n",
    "\n",
    "    def update_sugar_landscape(self):\n",
    "        self.sugar = np.zeros((self.height, self.width))\n",
    "        for center in self.job_centers:\n",
    "            x_grid, y_grid = np.meshgrid(np.arange(self.width), np.arange(self.height))\n",
    "            distance = np.sqrt((x_grid - center['x']) ** 2 + (y_grid - center['y']) ** 2)\n",
    "            sugar_level = center['max_sugar'] * np.exp(-distance ** 2 / (2 * self.params['sugar_peak_spread'] ** 2))\n",
    "            self.sugar += sugar_level\n",
    "        self.sugar = np.clip(self.sugar, 0, self.params['max_sugar'])\n",
    "        self.sugar = np.round(self.sugar).astype(int)\n",
    "\n",
    "    def initialize_agents(self):\n",
    "        agents = []\n",
    "        available_positions = set((x, y) for x in range(self.width) for y in range(self.height))\n",
    "        for i in range(self.num_agents):\n",
    "            if not available_positions:\n",
    "                break\n",
    "            x, y = available_positions.pop()\n",
    "            agents.append(self.create_agent(i, x, y))\n",
    "        return agents\n",
    "\n",
    "    def create_agent(self, id, x, y):\n",
    "        return {\n",
    "            'id': id, 'x': x, 'y': y,\n",
    "            'sugar': np.random.randint(40, 80),\n",
    "            'metabolism': np.random.randint(1, 3),\n",
    "            'vision': np.random.randint(1, self.params['vision_range'] + 1),\n",
    "            'broadcast_radius': max(1, int(np.random.normal(self.broadcast_radius, self.broadcast_radius / 3))),\n",
    "            'messages': deque(maxlen=100),\n",
    "            'destination': None,\n",
    "            'done': False\n",
    "        }\n",
    "\n",
    "    def get_agent_observation(self, agent):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        obs_range = self.params['vision_range']\n",
    "        sugar_obs = self.sugar[max(0, y - obs_range):min(self.height, y + obs_range + 1),\n",
    "                               max(0, x - obs_range):min(self.width, x + obs_range + 1)]\n",
    "        pad_width_x = (max(0, obs_range - x), max(0, x + obs_range + 1 - self.width))\n",
    "        pad_width_y = (max(0, obs_range - y), max(0, y + obs_range + 1 - self.height))\n",
    "        sugar_obs = np.pad(sugar_obs, (pad_width_y, pad_width_x), mode='constant', constant_values=0)\n",
    "        sugar_obs = sugar_obs.flatten()\n",
    "        agent_features = np.array([agent['sugar'], agent['metabolism']])\n",
    "        observation = np.concatenate((sugar_obs, agent_features))\n",
    "        return observation\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, max(0, y - 1)),        # Up\n",
    "            1: (x, min(self.height - 1, y + 1)),  # Down\n",
    "            2: (max(0, x - 1), y),        # Left\n",
    "            3: (min(self.width - 1, x + 1), y),   # Right\n",
    "            4: (x, y)                     # Stay\n",
    "        }\n",
    "        new_x, new_y = possible_moves[action]\n",
    "        if (new_x, new_y) not in self.agent_positions:\n",
    "            self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            agent['x'], agent['y'] = new_x, new_y\n",
    "            self.agent_positions.add((new_x, new_y))\n",
    "\n",
    "    def step(self):\n",
    "        for center in self.job_centers:\n",
    "            center['duration'] -= 1\n",
    "        self.job_centers = [center for center in self.job_centers if center['duration'] > 0]\n",
    "        if np.random.random() < self.params['sugar_peak_frequency']:\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "        observations = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        values = []\n",
    "\n",
    "        for agent in self.agents:\n",
    "            observation = self.get_agent_observation(agent)\n",
    "            action, log_prob, value = self.agent.select_action(observation)\n",
    "            self.move_agent(agent, action)\n",
    "            collected_sugar = self.sugar[agent['y'], agent['x']]\n",
    "            agent['sugar'] += collected_sugar\n",
    "            self.sugar[agent['y'], agent['x']] = 0\n",
    "            agent['sugar'] -= agent['metabolism']\n",
    "            reward = collected_sugar - agent['metabolism']  # Reward is net sugar gain\n",
    "            done = agent['sugar'] <= 0\n",
    "\n",
    "            self.agent.store_experience(observation, action, log_prob, reward, done, value.item())\n",
    "\n",
    "            agent['done'] = done  # Update done flag\n",
    "\n",
    "        alive_agents = []\n",
    "        for agent in self.agents:\n",
    "            if agent['done']:\n",
    "                self.dead_agents.append({'x': agent['x'], 'y': agent['y'], 'death_time': self.timestep})\n",
    "                self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            else:\n",
    "                alive_agents.append(agent)\n",
    "        self.agents = alive_agents\n",
    "\n",
    "        self.dead_agents = [agent for agent in self.dead_agents if self.timestep - agent['death_time'] <= 5]\n",
    "\n",
    "        if len(self.agent.memory['rewards']) >= self.params['batch_size']:\n",
    "            self.agent.update_policy()\n",
    "\n",
    "        self.collect_data()\n",
    "        self.timestep += 1\n",
    "\n",
    "    def train(self, num_episodes=2000):\n",
    "        start_time = time.time()\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            self.step()\n",
    "            if episode % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                estimated_time_per_episode = elapsed_time / episode\n",
    "                remaining_time = estimated_time_per_episode * (num_episodes - episode)\n",
    "                print(f\"Episode {episode}/{num_episodes}, Estimated remaining time: {remaining_time:.2f} seconds\")\n",
    "                self.agent.update_policy()\n",
    "\n",
    "    def render(self):\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                sugar_level = self.sugar[y, x]\n",
    "                color = self.get_color(sugar_level)\n",
    "                pygame.draw.rect(self.screen, color,\n",
    "                                 (x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "                if self.show_sugar_levels:\n",
    "                    sugar_text = self.font.render(f\"{sugar_level}\", True, (0, 0, 0))\n",
    "                    text_rect = sugar_text.get_rect(center=(x * self.cell_size + self.cell_size // 2,\n",
    "                                                            y * self.cell_size + self.cell_size // 2))\n",
    "                    self.screen.blit(sugar_text, text_rect)\n",
    "\n",
    "        for dead_agent in self.dead_agents:\n",
    "            pygame.draw.circle(self.screen, (128, 128, 128),\n",
    "                               (int(dead_agent['x'] * self.cell_size + self.cell_size / 2),\n",
    "                                int(dead_agent['y'] * self.cell_size + self.cell_size / 2)),\n",
    "                               int(self.cell_size / 3))\n",
    "\n",
    "        for agent in self.agents:\n",
    "            if self.show_broadcast_radius:\n",
    "                pygame.draw.circle(self.screen, (200, 200, 200),\n",
    "                                   (int(agent['x'] * self.cell_size + self.cell_size / 2),\n",
    "                                    int(agent['y'] * self.cell_size + self.cell_size / 2)),\n",
    "                                   int(agent['broadcast_radius'] * self.cell_size), 1)\n",
    "\n",
    "            pygame.draw.circle(self.screen, (255, 0, 0),\n",
    "                               (int(agent['x'] * self.cell_size + self.cell_size / 2),\n",
    "                                int(agent['y'] * self.cell_size + self.cell_size / 2)),\n",
    "                               int(self.cell_size / 3))\n",
    "\n",
    "            if self.show_agent_paths and agent['destination']:\n",
    "                pygame.draw.line(self.screen, (0, 255, 0),\n",
    "                                 (int(agent['x'] * self.cell_size + self.cell_size / 2),\n",
    "                                  int(agent['y'] * self.cell_size + self.cell_size / 2)),\n",
    "                                 (int(agent['destination'][0] * self.cell_size + self.cell_size / 2),\n",
    "                                  int(agent['destination'][1] * self.cell_size + self.cell_size / 2)),\n",
    "                                 1)\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def get_color(self, sugar_level):\n",
    "        if sugar_level == 0:\n",
    "            return (255, 255, 255)\n",
    "        else:\n",
    "            intensity = sugar_level / self.params['max_sugar']\n",
    "            return (255, 255, int(255 * (1 - intensity)))\n",
    "\n",
    "    def collect_data(self):\n",
    "        population = len(self.agents)\n",
    "        total_wealth = sum(agent['sugar'] for agent in self.agents)\n",
    "        average_wealth = total_wealth / population if population > 0 else 0\n",
    "\n",
    "        self.population_history.append(population)\n",
    "        self.average_wealth_history.append(average_wealth)\n",
    "        self.gini_coefficient_history.append(self.calculate_gini_coefficient())\n",
    "\n",
    "    def calculate_gini_coefficient(self):\n",
    "        if not self.agents:\n",
    "            return 0\n",
    "        wealth_values = sorted(agent['sugar'] for agent in self.agents)\n",
    "        cumulative_wealth = np.cumsum(wealth_values)\n",
    "        return (np.sum((2 * np.arange(1, len(wealth_values) + 1) - len(wealth_values) - 1) * wealth_values) /\n",
    "                (len(wealth_values) * np.sum(wealth_values)))\n",
    "\n",
    "    def plot_results(self):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.plot(self.population_history)\n",
    "        plt.title('Population over Time')\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Population')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.plot(self.average_wealth_history)\n",
    "        plt.title('Average Wealth over Time')\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Average Wealth')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.plot(self.gini_coefficient_history)\n",
    "        plt.title('Gini Coefficient over Time')\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Gini Coefficient')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def final_simulation(self, max_timesteps=1000):\n",
    "        running = True\n",
    "        while running and self.timestep < max_timesteps:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "\n",
    "            self.step()\n",
    "            self.render()\n",
    "            self.clock.tick(5)\n",
    "\n",
    "        self.plot_results()\n",
    "\n",
    "# Train the PPO agent without visualizing the environment\n",
    "env = SugarscapeEnvironment(width=50, height=50, num_agents=1000, cell_size=10,\n",
    "                            broadcast_radius=15, show_sugar_levels=False, show_broadcast_radius=False, show_agent_paths=False)\n",
    "env.train(num_episodes=2000)\n",
    "\n",
    "# Run the final simulation with rendering\n",
    "env.final_simulation(max_timesteps=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
