{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SugarscapeEnvironmentDQN' object has no attribute 'timestep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 451\u001b[0m\n\u001b[0;32m    440\u001b[0m MAX_TIMESTEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    441\u001b[0m env \u001b[38;5;241m=\u001b[39m SugarscapeEnvironmentDQN(\n\u001b[0;32m    442\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    443\u001b[0m     height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m     visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Disable visualization during training\u001b[39;00m\n\u001b[0;32m    450\u001b[0m )\n\u001b[1;32m--> 451\u001b[0m env\u001b[38;5;241m.\u001b[39mrun_training(total_episodes\u001b[38;5;241m=\u001b[39mTOTAL_EPISODES, max_timesteps\u001b[38;5;241m=\u001b[39mMAX_TIMESTEPS)\n\u001b[0;32m    452\u001b[0m env\u001b[38;5;241m.\u001b[39mplot_results()\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed and results saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 403\u001b[0m, in \u001b[0;36mSugarscapeEnvironmentDQN.run_training\u001b[1;34m(self, total_episodes, max_timesteps)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode \u001b[38;5;241m=\u001b[39m episode\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_timesteps):\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Episode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 291\u001b[0m, in \u001b[0;36mSugarscapeEnvironmentDQN.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_sugar_landscape()\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Update epsilon\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_end, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_start \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Broadcast messages\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbroadcast_messages()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SugarscapeEnvironmentDQN' object has no attribute 'timestep'"
     ]
    }
   ],
   "source": [
    "# sugarscape_dqn_training.py\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the DQN agent class\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class SugarscapeEnvironmentDQN:\n",
    "    def __init__(self, width=30, height=30, num_agents=400, cell_size=10, seed=42,\n",
    "                 max_messages=5, max_timesteps=1000, visualize=False):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_agents = num_agents\n",
    "        self.cell_size = cell_size\n",
    "        self.seed = seed\n",
    "        self.visualize = visualize\n",
    "        self.max_messages = max_messages\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        # Initialize random number generators with the seed for reproducibility\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        self.params = {\n",
    "            'max_sugar': 5,\n",
    "            'growth_rate': 1,\n",
    "            'sugar_peak_frequency': 0.05,\n",
    "            'sugar_peak_spread': 3,\n",
    "            'job_center_duration': (20, 50),\n",
    "            'vision_range': 1,  # Reduced vision range for simplicity\n",
    "            'message_expiry': 15,\n",
    "            'max_relay_messages': 10,\n",
    "            'exploration_probability': 0.1\n",
    "        }\n",
    "\n",
    "        self.job_centers = []\n",
    "        self.sugar = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.create_initial_sugar_peaks()\n",
    "        self.agents = self.initialize_agents()\n",
    "        self.agent_positions = set((agent['x'], agent['y']) for agent in self.agents)\n",
    "        self.dead_agents = []\n",
    "\n",
    "        # DQN Components\n",
    "        self.state_size = 5 + (2 * self.params['vision_range'] + 1) ** 2 + (3 * self.max_messages)\n",
    "        self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.q_network = DQNAgent(self.state_size, self.action_size)\n",
    "        self.target_network = DQNAgent(self.state_size, self.action_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=1e-3)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.replay_buffer = deque(maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.1\n",
    "        self.epsilon_decay = 10000\n",
    "        self.epsilon = self.epsilon_start\n",
    "        self.episode = 0  # For tracking episodes\n",
    "\n",
    "    def create_initial_sugar_peaks(self, num_peaks=2):\n",
    "        for _ in range(num_peaks):\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "    def create_job_center(self):\n",
    "        x, y = np.random.randint(0, self.width), np.random.randint(0, self.height)\n",
    "        duration = np.random.randint(*self.params['job_center_duration'])\n",
    "        self.job_centers.append({\n",
    "            'x': x, 'y': y,\n",
    "            'duration': duration,\n",
    "            'max_sugar': self.params['max_sugar']\n",
    "        })\n",
    "\n",
    "    def update_sugar_landscape(self):\n",
    "        self.sugar = np.zeros((self.height, self.width))\n",
    "        for center in self.job_centers:\n",
    "            x_grid, y_grid = np.meshgrid(np.arange(self.width), np.arange(self.height))\n",
    "            distance = np.sqrt((x_grid - center['x']) ** 2 + (y_grid - center['y']) ** 2)\n",
    "            sugar_level = center['max_sugar'] * np.exp(-distance ** 2 / (2 * self.params['sugar_peak_spread'] ** 2))\n",
    "            self.sugar += sugar_level\n",
    "        self.sugar = np.clip(self.sugar, 0, self.params['max_sugar'])\n",
    "        self.sugar = np.round(self.sugar).astype(int)\n",
    "\n",
    "    def initialize_agents(self):\n",
    "        agents = []\n",
    "        available_positions = set((x, y) for x in range(self.width) for y in range(self.height))\n",
    "        for i in range(self.num_agents):\n",
    "            if not available_positions:\n",
    "                break\n",
    "            x, y = available_positions.pop()\n",
    "            agents.append(self.create_agent(i, x, y))\n",
    "        return agents\n",
    "\n",
    "    def create_agent(self, id, x, y):\n",
    "        return {\n",
    "            'id': id,\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'sugar': np.random.randint(20, 50),\n",
    "            'metabolism': np.random.randint(1, 3),\n",
    "            'vision': self.params['vision_range'],\n",
    "            'messages': deque(maxlen=self.max_messages),\n",
    "            'destination': None,\n",
    "            'memory': deque(maxlen=10),\n",
    "            'path': [],\n",
    "            'age': 0\n",
    "        }\n",
    "\n",
    "    def get_state(self, agent):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        sugar = agent['sugar'] / 100  # Normalize sugar level\n",
    "        metabolism = agent['metabolism'] / 5  # Normalize metabolism\n",
    "        vision = agent['vision'] / 5  # Normalize vision\n",
    "\n",
    "        # Extract sugar levels within vision range\n",
    "        vision_range = agent['vision']\n",
    "        y_min = max(0, y - vision_range)\n",
    "        y_max = min(self.height, y + vision_range + 1)\n",
    "        x_min = max(0, x - vision_range)\n",
    "        x_max = min(self.width, x + vision_range + 1)\n",
    "        sugar_map = self.sugar[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # Pad the sugar map to a fixed size\n",
    "        expected_size = (2 * vision_range + 1, 2 * vision_range + 1)\n",
    "        padded_sugar_map = np.zeros(expected_size)\n",
    "        y_offset = y_min - (y - vision_range)\n",
    "        x_offset = x_min - (x - vision_range)\n",
    "        y_start = y_offset\n",
    "        y_end = y_start + sugar_map.shape[0]\n",
    "        x_start = x_offset\n",
    "        x_end = x_start + sugar_map.shape[1]\n",
    "        padded_sugar_map[y_start:y_end, x_start:x_end] = sugar_map\n",
    "        sugar_map_flat = padded_sugar_map.flatten() / self.params['max_sugar']  # Normalize sugar levels\n",
    "\n",
    "        # Encode messages\n",
    "        N = self.max_messages  # Number of messages to encode\n",
    "        messages = list(agent['messages'])[-N:]  # Get the last N messages\n",
    "        message_features = []\n",
    "        for msg in messages:\n",
    "            # Normalize message coordinates relative to grid size\n",
    "            msg_x = msg['x'] / self.width\n",
    "            msg_y = msg['y'] / self.height\n",
    "            msg_sugar = msg['sugar_amount'] / self.params['max_sugar']\n",
    "            message_features.extend([msg_x, msg_y, msg_sugar])\n",
    "        # Pad remaining messages with zeros if fewer than N\n",
    "        while len(message_features) < 3 * N:\n",
    "            message_features.extend([0.0, 0.0, 0.0])\n",
    "\n",
    "        state = np.concatenate((\n",
    "            [x / self.width, y / self.height, sugar, metabolism, vision],\n",
    "            sugar_map_flat,\n",
    "            message_features\n",
    "        ))\n",
    "        return state\n",
    "\n",
    "    def select_action(self, state, valid_actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            # Mask invalid actions by setting their Q-values to -inf\n",
    "            mask = torch.full((self.action_size,), -np.inf)\n",
    "            mask[valid_actions] = 0\n",
    "            masked_q = q_values + torch.FloatTensor(mask)\n",
    "            return masked_q.argmax().item()\n",
    "\n",
    "    def get_valid_actions(self, agent):\n",
    "        actions = []\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, y - 1),  # Up\n",
    "            1: (x, y + 1),  # Down\n",
    "            2: (x - 1, y),  # Left\n",
    "            3: (x + 1, y),  # Right\n",
    "            4: (x, y)       # Stay\n",
    "        }\n",
    "        for action, (nx, ny) in possible_moves.items():\n",
    "            if 0 <= nx < self.width and 0 <= ny < self.height:\n",
    "                if (nx, ny) not in self.agent_positions or (nx, ny) == (x, y):\n",
    "                    actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        possible_moves = {\n",
    "            0: (x, y - 1),  # Up\n",
    "            1: (x, y + 1),  # Down\n",
    "            2: (x - 1, y),  # Left\n",
    "            3: (x + 1, y),  # Right\n",
    "            4: (x, y)       # Stay\n",
    "        }\n",
    "        nx, ny = possible_moves[action]\n",
    "        if (0 <= nx < self.width and 0 <= ny < self.height and\n",
    "                ((nx, ny) not in self.agent_positions or (nx, ny) == (x, y))):\n",
    "            self.agent_positions.remove((x, y))\n",
    "            agent['x'], agent['y'] = nx, ny\n",
    "            agent['path'].append((agent['x'], agent['y']))\n",
    "            self.agent_positions.add((nx, ny))\n",
    "\n",
    "    def collect_sugar_and_update_agent(self, agent):\n",
    "        collected_sugar = self.sugar[agent['y'], agent['x']]\n",
    "        agent['sugar'] += collected_sugar\n",
    "        self.sugar[agent['y'], agent['x']] = 0\n",
    "        agent['sugar'] -= agent['metabolism']\n",
    "        agent['age'] += 1\n",
    "\n",
    "    def broadcast_messages(self):\n",
    "        if not self.agents:\n",
    "            return  # No agents to broadcast\n",
    "\n",
    "        positions = np.array([[agent['x'], agent['y']] for agent in self.agents])\n",
    "        tree = cKDTree(positions)\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            # Identify visible sugar peaks\n",
    "            visible_sugar = self.get_visible_sugar(agent)\n",
    "            sugar_locations = np.argwhere(visible_sugar > 0)\n",
    "            messages = []\n",
    "            for loc in sugar_locations:\n",
    "                msg_x = agent['x'] + loc[1] - agent['vision']\n",
    "                msg_y = agent['y'] + loc[0] - agent['vision']\n",
    "                # Ensure message coordinates are within grid\n",
    "                msg_x = int(np.clip(msg_x, 0, self.width - 1))\n",
    "                msg_y = int(np.clip(msg_y, 0, self.height - 1))\n",
    "                msg = {\n",
    "                    'sender_id': agent['id'],\n",
    "                    'timestep': self.timestep,\n",
    "                    'sugar_amount': self.sugar[msg_y, msg_x],\n",
    "                    'x': msg_x,\n",
    "                    'y': msg_y\n",
    "                }\n",
    "                messages.append(msg)\n",
    "\n",
    "            # Broadcast to neighbors within broadcast_radius\n",
    "            radius = 5  # Fixed broadcast radius\n",
    "            neighbors = tree.query_ball_point([agent['x'], agent['y']], radius)\n",
    "            for neighbor_idx in neighbors:\n",
    "                if neighbor_idx != i:\n",
    "                    for msg in messages:\n",
    "                        self.agents[neighbor_idx]['messages'].append(msg)\n",
    "\n",
    "    def get_visible_sugar(self, agent):\n",
    "        x, y = agent['x'], agent['y']\n",
    "        vision = agent['vision']\n",
    "        y_min = max(0, y - vision)\n",
    "        y_max = min(self.height, y + vision + 1)\n",
    "        x_min = max(0, x - vision)\n",
    "        x_max = min(self.width, x + vision + 1)\n",
    "        visible_area = self.sugar[y_min:y_max, x_min:x_max]\n",
    "        return visible_area\n",
    "\n",
    "    def step(self):\n",
    "        # Update job centers and sugar landscape\n",
    "        for center in self.job_centers:\n",
    "            center['duration'] -= 1\n",
    "        self.job_centers = [center for center in self.job_centers if center['duration'] > 0]\n",
    "        if np.random.random() < self.params['sugar_peak_frequency']:\n",
    "            self.create_job_center()\n",
    "        self.update_sugar_landscape()\n",
    "\n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_start - self.timestep / self.epsilon_decay)\n",
    "\n",
    "        # Broadcast messages\n",
    "        self.broadcast_messages()\n",
    "\n",
    "        # For each agent, select action and collect experience\n",
    "        for agent in self.agents:\n",
    "            state = self.get_state(agent)\n",
    "            valid_actions = self.get_valid_actions(agent)\n",
    "            if not valid_actions:\n",
    "                continue  # Skip if no valid actions\n",
    "            action = self.select_action(state, valid_actions)\n",
    "            prev_sugar = agent['sugar']\n",
    "            self.move_agent(agent, action)\n",
    "            self.collect_sugar_and_update_agent(agent)\n",
    "            next_state = self.get_state(agent)\n",
    "            reward = agent['sugar'] - prev_sugar  # Reward is the change in sugar\n",
    "            done = agent['sugar'] <= 0\n",
    "            self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if len(self.replay_buffer) >= self.batch_size:\n",
    "                loss = self.train_dqn()\n",
    "                # Print progress every 100 episodes\n",
    "                if (self.episode + 1) % 100 == 0:\n",
    "                    print(f\"Episode: {self.episode + 1}, Timestep: {self.timestep}, Loss: {loss:.4f}, Epsilon: {self.epsilon:.4f}\")\n",
    "                    # Print a summary of weights (e.g., mean and std of first layer)\n",
    "                    first_layer_weights = self.q_network.fc1.weight.data.numpy()\n",
    "                    print(f\"First Layer Weights - Mean: {first_layer_weights.mean():.4f}, Std: {first_layer_weights.std():.4f}\")\n",
    "\n",
    "        # Handle agent death\n",
    "        alive_agents = []\n",
    "        for agent in self.agents:\n",
    "            if agent['sugar'] <= 0:\n",
    "                self.dead_agents.append({'x': agent['x'], 'y': agent['y'], 'death_time': self.timestep})\n",
    "                self.agent_positions.remove((agent['x'], agent['y']))\n",
    "            else:\n",
    "                alive_agents.append(agent)\n",
    "        self.agents = alive_agents\n",
    "\n",
    "        # Replenish agents\n",
    "        self.replenish_agents()\n",
    "\n",
    "        self.dead_agents = [agent for agent in self.dead_agents if self.timestep - agent['death_time'] <= 5]\n",
    "\n",
    "        self.collect_data()\n",
    "        self.timestep += 1\n",
    "\n",
    "        # Save model every 100 episodes\n",
    "        if (self.episode + 1) % 100 == 0:\n",
    "            torch.save(self.q_network.state_dict(), f'dqn_q_network_episode_{self.episode + 1}.pth')\n",
    "            print(f\"Model saved at episode {self.episode + 1}\")\n",
    "\n",
    "    def train_dqn(self):\n",
    "        experiences = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_network(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q = rewards + (self.gamma * max_next_q * (1 - dones))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(current_q, target_q)\n",
    "\n",
    "        # Optimize the Q-network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def replenish_agents(self):\n",
    "        while len(self.agents) < self.num_agents:\n",
    "            x, y = random.randint(0, self.width - 1), random.randint(0, self.height - 1)\n",
    "            if (x, y) not in self.agent_positions:\n",
    "                agent_id = max([agent['id'] for agent in self.agents] + [0]) + 1\n",
    "                new_agent = self.create_agent(agent_id, x, y)\n",
    "                self.agent_positions.add((x, y))\n",
    "                self.agents.append(new_agent)\n",
    "\n",
    "    def collect_data(self):\n",
    "        population = len(self.agents)\n",
    "        total_wealth = sum(agent['sugar'] for agent in self.agents)\n",
    "        average_wealth = total_wealth / population if population > 0 else 0\n",
    "\n",
    "        self.population_history.append(population)\n",
    "        self.average_wealth_history.append(average_wealth)\n",
    "        self.gini_coefficient_history.append(self.calculate_gini_coefficient())\n",
    "\n",
    "    def calculate_gini_coefficient(self):\n",
    "        if not self.agents:\n",
    "            return 0\n",
    "        wealth_values = sorted(agent['sugar'] for agent in self.agents)\n",
    "        cumulative_wealth = np.cumsum(wealth_values)\n",
    "        n = len(wealth_values)\n",
    "        gini = (n + 1 - 2 * np.sum(cumulative_wealth) / cumulative_wealth[-1]) / n\n",
    "        return gini\n",
    "\n",
    "    def run_training(self, total_episodes=1000, max_timesteps=1000):\n",
    "        for episode in range(total_episodes):\n",
    "            self.episode = episode\n",
    "            for _ in range(max_timesteps):\n",
    "                self.step()\n",
    "            print(f\"Completed Episode: {episode + 1}\")\n",
    "\n",
    "        # Save the final model\n",
    "        torch.save(self.q_network.state_dict(), 'dqn_q_network_final.pth')\n",
    "        torch.save(self.target_network.state_dict(), 'dqn_target_network_final.pth')\n",
    "        np.save('dqn_population.npy', self.population_history)\n",
    "        np.save('dqn_average_wealth.npy', self.average_wealth_history)\n",
    "        np.save('dqn_gini_coefficient.npy', self.gini_coefficient_history)\n",
    "\n",
    "    def plot_results(self):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.plot(self.population_history)\n",
    "        plt.title('Population over Time (DQN)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Population')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.plot(self.average_wealth_history)\n",
    "        plt.title('Average Wealth over Time (DQN)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Wealth')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.plot(self.gini_coefficient_history)\n",
    "        plt.title('Gini Coefficient over Time (DQN)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Gini Coefficient')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage without visualization\n",
    "if __name__ == \"__main__\":\n",
    "    TOTAL_EPISODES = 1000\n",
    "    MAX_TIMESTEPS = 1000\n",
    "    env = SugarscapeEnvironmentDQN(\n",
    "        width=30,\n",
    "        height=30,\n",
    "        num_agents=400,\n",
    "        cell_size=10,\n",
    "        seed=42,\n",
    "        max_messages=5,\n",
    "        max_timesteps=MAX_TIMESTEPS,\n",
    "        visualize=False  # Disable visualization during training\n",
    "    )\n",
    "    env.run_training(total_episodes=TOTAL_EPISODES, max_timesteps=MAX_TIMESTEPS)\n",
    "    env.plot_results()\n",
    "    print(\"\\nTraining completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
